#  Entity Extraction Pipeline (GLiNER + Airflow)

This repository contains an end-to-end entity extraction pipeline that processes documents to extract and match named entities using a fine-tuned deep learning model. It leverages **spaCy**, **GLiNER**, **Airflow**, **Docker**, and **Parquet/SQLite** for scalable, modular, and efficient processing.

---

##  Features

- âœ… Named Entity Recognition with **GLiNER**
- âœ… Dockerized NER microservice for isolation and caching
- âœ… Entity matching with alias disambiguation
- âœ… DAG-based orchestration using **Apache Airflow**
- âœ… Persistent model cache via volume mounting
- âœ… Intermediate and final results stored as **Parquet** (or SQLite)
- âœ… Fully extensible for online inference or REST APIs

---

## Architecture

```mermaid
graph TD
    A[Load Documents (CSV)] --> B[Entity Extraction (Docker)]
    B --> C[Entity Matching (Alias Table)]
    C --> D[Store to Parquet/SQLite]

entity-extraction-pipeline-impl/
â”‚
â”œâ”€â”€ dags/                      # Airflow DAGs
      â”œâ”€â”€ dag_service.py       # DAG starter service
      â”œâ”€â”€ data_processing_service.py
      â”œâ”€â”€ data_matching_service.py   
â”œâ”€â”€ docker_service/          # Docker context for NER service
      â”œâ”€â”€ extractor.py       
      â”œâ”€â”€ Dockerfile
      â”œâ”€â”€ run_extractor.py   # Docker entrypoint: NER + output
      â””â”€â”€ requirements.txt       
â”œâ”€â”€ shared_volume/           # Input document CSVs shared accross docker
â”œâ”€â”€ extracted_entity/        # extracted Input JSON files received from docker
â”œâ”€â”€ model_cache/             # Cached GLiNER models (mounted)
â”œâ”€â”€ data/                    # Source document and alias CSVs
â”œâ”€â”€ output/                  # Output result to be stored in DB
â”œâ”€â”€ docker-compose.yml

**DAG & Docker service are build in different directory, so that its easy for docker_service to be scaled in future

## How It Works
load_documents: Loads document CSV and saves a timestamped copy to shared_volume/.

build_docker_command: Constructs the Docker Compose command with dynamic file paths.

run_entity_extraction: Executes the GLiNER NER model in a Docker container using mounted volumes.

match_entities: Matches extracted entities against aliases (from entity_aliases.csv).

store_entities: Saves final structured output as a Parquet file.

## Setup & Run
1. Build Docker Image
docker-compose build ner_extractor

2. Start Airflow
export AIRFLOW_HOME=$(pwd)/airflow_home
airflow db init
airflow webserver --port 8080
# in another terminal
airflow scheduler
                OR
uv run --env-file .env airflow standalone

3. Trigger DAG
via UI

<img width="1488" alt="Screenshot 2025-06-06 at 10 36 36â€¯AM" src="https://github.com/user-attachments/assets/45e7b761-7649-4bf7-becd-1068f5bac491" />

ðŸ§ª Sample Command for Manual Run
docker-compose run --rm ner_extractor /shared/input_test.csv /app/output/output_test.json

## Model Caching
Model is cached at: /root/.cache/huggingface/
Mapped to host via: - ./model_cache:/root/.cache/huggingface
--This ensures fast reuse of GLiNER weights without redownloading.

<img width="1029" alt="Screenshot 2025-06-06 at 10 41 00â€¯AM" src="https://github.com/user-attachments/assets/713b1da1-9434-4117-8706-027d68373f4d" />

shows cache logs during task execution.

## Output Format
-uuid (from document)

-title, content, date, etc. (from document)

-entities: a list of:       (from extracted entity & aliases table)

  -entity_text, entity_type, start_pos, end_pos

  -is_matched, matched_entity_id, matched_entity_name




